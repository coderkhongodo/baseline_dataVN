#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script Ä‘á»ƒ chuáº©n bá»‹ dá»¯ liá»‡u training vá»›i chá»‰ cÃ¡c cá»™t cáº§n thiáº¿t: id, title, label
PhÃ¹ há»£p cho viá»‡c huáº¥n luyá»‡n model phÃ¢n loáº¡i clickbait tiáº¿ng Viá»‡t
"""

import json
import os
import argparse
from pathlib import Path
import pandas as pd
from collections import Counter

def extract_training_data(input_file, output_file):
    """
    Extract chá»‰ cÃ¡c cá»™t cáº§n thiáº¿t tá»« file dá»¯ liá»‡u gá»‘c
    """
    print(f"ğŸ” Äang xá»­ lÃ½ file: {input_file}")
    
    training_data = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                item = json.loads(line.strip())
                
                # Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t cáº§n thiáº¿t
                training_item = {
                    "id": item.get("id", f"item_{line_num}"),
                    "title": item.get("title", ""),
                    "label": item.get("label", "non-clickbait")
                }
                
                # Kiá»ƒm tra dá»¯ liá»‡u há»£p lá»‡
                if training_item["title"].strip():
                    training_data.append(training_item)
                else:
                    print(f"âš ï¸  DÃ²ng {line_num}: Title rá»—ng, bá» qua")
                    
            except json.JSONDecodeError as e:
                print(f"âŒ Lá»—i JSON á»Ÿ dÃ²ng {line_num}: {e}")
                continue
    
    # LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in training_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… ÄÃ£ lÆ°u {len(training_data)} máº«u vÃ o {output_file}")
    
    # Thá»‘ng kÃª
    labels = [item["label"] for item in training_data]
    label_counts = Counter(labels)
    print(f"ğŸ“Š PhÃ¢n phá»‘i label:")
    for label, count in label_counts.items():
        percentage = (count / len(training_data)) * 100
        print(f"  {label}: {count} ({percentage:.1f}%)")
    
    return training_data

def check_data_quality(data_file):
    """
    Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u
    """
    print(f"\nğŸ” Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u: {data_file}")
    
    issues = []
    valid_count = 0
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                item = json.loads(line.strip())
                
                # Kiá»ƒm tra cÃ¡c trÆ°á»ng báº¯t buá»™c
                if not item.get("id"):
                    issues.append(f"DÃ²ng {line_num}: Thiáº¿u ID")
                    continue
                    
                if not item.get("title", "").strip():
                    issues.append(f"DÃ²ng {line_num}: Title rá»—ng")
                    continue
                    
                if item.get("label") not in ["clickbait", "non-clickbait"]:
                    issues.append(f"DÃ²ng {line_num}: Label khÃ´ng há»£p lá»‡ '{item.get('label')}'")
                    continue
                
                # Kiá»ƒm tra Ä‘á»™ dÃ i title
                title_len = len(item["title"])
                if title_len < 10:
                    issues.append(f"DÃ²ng {line_num}: Title quÃ¡ ngáº¯n ({title_len} kÃ½ tá»±)")
                elif title_len > 200:
                    issues.append(f"DÃ²ng {line_num}: Title quÃ¡ dÃ i ({title_len} kÃ½ tá»±)")
                
                valid_count += 1
                
            except json.JSONDecodeError:
                issues.append(f"DÃ²ng {line_num}: Lá»—i JSON")
    
    print(f"âœ… Máº«u há»£p lá»‡: {valid_count}")
    if issues:
        print(f"âš ï¸  PhÃ¡t hiá»‡n {len(issues)} váº¥n Ä‘á»:")
        for issue in issues[:10]:  # Chá»‰ hiá»ƒn thá»‹ 10 váº¥n Ä‘á» Ä‘áº§u
            print(f"   {issue}")
        if len(issues) > 10:
            print(f"   ... vÃ  {len(issues) - 10} váº¥n Ä‘á» khÃ¡c")
    else:
        print("ğŸ‰ Dá»¯ liá»‡u hoÃ n toÃ n sáº¡ch!")

def create_vocab_analysis(data_files):
    """
    PhÃ¢n tÃ­ch tá»« vá»±ng Ä‘á»ƒ setup tokenizer
    """
    print(f"\nğŸ“ PhÃ¢n tÃ­ch tá»« vá»±ng...")
    
    all_titles = []
    total_chars = 0
    max_length = 0
    min_length = float('inf')
    
    for data_file in data_files:
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line.strip())
                    title = item.get("title", "")
                    if title:
                        all_titles.append(title)
                        title_len = len(title)
                        total_chars += title_len
                        max_length = max(max_length, title_len)
                        min_length = min(min_length, title_len)
                except:
                    continue
    
    if all_titles:
        avg_length = total_chars / len(all_titles)
        
        print(f"ğŸ“Š Thá»‘ng kÃª Ä‘á»™ dÃ i title:")
        print(f"  Tá»•ng sá»‘: {len(all_titles)} titles")
        print(f"  Äá»™ dÃ i trung bÃ¬nh: {avg_length:.1f} kÃ½ tá»±")
        print(f"  Äá»™ dÃ i min: {min_length} kÃ½ tá»±")
        print(f"  Äá»™ dÃ i max: {max_length} kÃ½ tá»±")
        
        # PhÃ¢n tÃ­ch phÃ¢n phá»‘i
        lengths = [len(title) for title in all_titles]
        lengths.sort()
        
        percentiles = [50, 75, 90, 95, 99]
        print(f"  PhÃ¢n phá»‘i Ä‘á»™ dÃ i:")
        for p in percentiles:
            idx = int(len(lengths) * p / 100)
            print(f"    {p}%: {lengths[idx]} kÃ½ tá»±")
        
        # Äá» xuáº¥t max_length cho tokenizer
        recommended_length = lengths[int(len(lengths) * 0.95)]  # 95th percentile
        token_length = int(recommended_length * 1.5)  # Æ¯á»›c lÆ°á»£ng sá»‘ token
        
        print(f"\nğŸ’¡ Äá» xuáº¥t cáº¥u hÃ¬nh:")
        print(f"  max_length cho tokenizer: {min(512, token_length)} tokens")
        print(f"  Sáº½ cover ~95% dá»¯ liá»‡u")

def process_all_splits(data_dir="data"):
    """
    Xá»­ lÃ½ táº¥t cáº£ cÃ¡c split dá»¯ liá»‡u
    """
    print("ğŸš€ Báº¯t Ä‘áº§u chuáº©n bá»‹ dá»¯ liá»‡u training")
    print("=" * 60)
    
    splits = ["train_dtVN", "val_dtVN", "test_dtVN"]
    processed_files = []
    
    for split in splits:
        input_file = Path(data_dir) / split / "data.jsonl"
        output_file = Path(data_dir) / split / "training_data.jsonl"
        
        if input_file.exists():
            print(f"\nğŸ“ Xá»­ lÃ½ {split}:")
            extract_training_data(input_file, output_file)
            check_data_quality(output_file)
            processed_files.append(output_file)
        else:
            print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y file: {input_file}")
    
    # PhÃ¢n tÃ­ch tá»« vá»±ng
    if processed_files:
        create_vocab_analysis(processed_files)
    
    print("\nğŸ‰ HoÃ n thÃ nh chuáº©n bá»‹ dá»¯ liá»‡u!")
    print("=" * 60)
    print("ğŸ“ Files Ä‘Ã£ táº¡o:")
    for split in splits:
        training_file = Path(data_dir) / split / "training_data.jsonl"
        if training_file.exists():
            print(f"  âœ… {training_file}")
    
    print(f"\nğŸ“‹ Cáº¥u trÃºc dá»¯ liá»‡u training:")
    print(f"  {{")
    print(f"    \"id\": \"article_xxxx\",")
    print(f"    \"title\": \"TiÃªu Ä‘á» bÃ i bÃ¡o tiáº¿ng Viá»‡t\",")
    print(f"    \"label\": \"clickbait\" hoáº·c \"non-clickbait\"")
    print(f"  }}")

def main():
    parser = argparse.ArgumentParser(description="Chuáº©n bá»‹ dá»¯ liá»‡u training clickbait")
    parser.add_argument("--data-dir", "-d", default="data",
                       help="ThÆ° má»¥c chá»©a dá»¯ liá»‡u (default: data)")
    parser.add_argument("--single-file", "-f", 
                       help="Xá»­ lÃ½ má»™t file duy nháº¥t")
    parser.add_argument("--output", "-o",
                       help="File output (chá»‰ khi dÃ¹ng --single-file)")
    
    args = parser.parse_args()
    
    if args.single_file:
        if not args.output:
            args.output = args.single_file.replace('.jsonl', '_training.jsonl')
        
        print(f"ğŸ” Xá»­ lÃ½ file Ä‘Æ¡n: {args.single_file}")
        extract_training_data(args.single_file, args.output)
        check_data_quality(args.output)
        create_vocab_analysis([args.output])
    else:
        process_all_splits(args.data_dir)

if __name__ == "__main__":
    main() 