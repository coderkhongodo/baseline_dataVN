#!/usr/bin/env python3
"""
Quick Inference Test for Vietnamese Clickbait Classification
Test nhanh inference trÃªn CPU vá»›i pre-trained model
"""

import os
import sys
import json
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def test_sample_texts():
    """Test vá»›i má»™t sá»‘ cÃ¢u máº«u"""
    sample_texts = [
        # Clickbait examples
        "Báº¡n sáº½ khÃ´ng tin ná»•i Ä‘iá»u gÃ¬ xáº£y ra tiáº¿p theo!",
        "10 bÃ­ máº­t mÃ  chá»‰ cÃ³ 1% dÃ¢n sá»‘ biáº¿t",
        "CÃ¡ch kiáº¿m tiá»n online mÃ  99% ngÆ°á»i khÃ´ng biáº¿t",
        "Sá»± tháº­t vá» cÄƒn bá»‡nh nÃ y sáº½ khiáº¿n báº¡n sá»‘c",
        
        # Non-clickbait examples  
        "Thá»§ tÆ°á»›ng chá»§ trÃ¬ há»p ChÃ­nh phá»§ thÆ°á»ng ká»³",
        "GiÃ¡ xÄƒng tÄƒng 500 Ä‘á»“ng tá»« 15h chiá»u nay",
        "Äáº¡i há»c Quá»‘c gia HÃ  Ná»™i tuyá»ƒn sinh 2024",
        "Dá»± bÃ¡o thá»i tiáº¿t: Báº¯c Bá»™ cÃ³ mÆ°a rÃ o vÃ  dÃ´ng"
    ]
    
    expected_labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 1=clickbait, 0=non-clickbait
    
    return sample_texts, expected_labels

def load_test_data_subset(file_path, n_samples=20):
    """Load subset of real test data"""
    if not os.path.exists(file_path):
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
        return [], []
    
    print(f"Loading test data tá»« {file_path}...")
    
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= n_samples:
                break
            data.append(json.loads(line.strip()))
    
    texts = [item['title'] for item in data]
    # Convert string labels to integers: "clickbait" -> 1, "non-clickbait" -> 0
    labels = [1 if item['label'] == 'clickbait' else 0 for item in data]
    
    return texts, labels

def test_model_inference(model_name, texts, true_labels=None):
    """Test model inference"""
    print(f"\nğŸ¤– Testing model: {model_name}")
    
    try:
        # Load model
        print("ğŸ“¥ Loading tokenizer and model...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        print(f"âœ… Model loaded successfully")
        print(f"ğŸ“Š Model parameters: {model.num_parameters():,}")
        print(f"âš ï¸ Note: Using pre-trained model (chÆ°a fine-tune), predictions cÃ³ thá»ƒ random")
        
        # Manual inference (khÃ´ng dÃ¹ng pipeline vÃ¬ model chÆ°a fine-tune)
        print(f"\nğŸ” Running manual inference on {len(texts)} texts...")
        start_time = datetime.now()
        
        predictions = []
        confidences = []
        
        for i, text in enumerate(texts):
            # Tokenize
            inputs = tokenizer(
                text[:128], 
                truncation=True, 
                padding=True, 
                return_tensors="pt"
            )
            
            # Forward pass
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                
            # Convert to probabilities
            probs = torch.softmax(logits, dim=-1)
            
            # Get prediction and confidence
            pred_idx = torch.argmax(probs, dim=-1).item()
            confidence = torch.max(probs, dim=-1).values.item()
            
            predictions.append(pred_idx)
            confidences.append(confidence)
            
            if (i + 1) % 10 == 0:
                print(f"   Processed {i + 1}/{len(texts)} texts...")
        
        end_time = datetime.now()
        inference_time = end_time - start_time
        
        print(f"â° Inference completed in: {inference_time}")
        print(f"ğŸš€ Speed: {len(texts) / inference_time.total_seconds():.2f} texts/second")
        
        # Calculate accuracy if true labels provided
        accuracy = None
        if true_labels:
            correct = sum(1 for p, t in zip(predictions, true_labels) if p == t)
            accuracy = correct / len(true_labels)
            print(f"ğŸ¯ Accuracy: {accuracy:.4f} ({correct}/{len(true_labels)})")
            print(f"âš ï¸ Note: Pre-trained model chÆ°a fine-tune cho clickbait, accuracy cÃ³ thá»ƒ tháº¥p")
        
        # Show predictions
        print(f"\nğŸ“‹ Sample Predictions:")
        for i in range(min(10, len(texts))):
            text = texts[i][:60] + "..." if len(texts[i]) > 60 else texts[i]
            pred_label = "Clickbait" if predictions[i] == 1 else "Non-Clickbait"
            conf = confidences[i]
            
            status = ""
            if true_labels:
                true_label = "Clickbait" if true_labels[i] == 1 else "Non-Clickbait"
                status = "âœ…" if predictions[i] == true_labels[i] else "âŒ"
                print(f"   {status} Text: {text}")
                print(f"      True: {true_label}, Pred: {pred_label} ({conf:.3f})")
            else:
                print(f"   Text: {text}")
                print(f"   Pred: {pred_label} ({conf:.3f})")
        
        return predictions, confidences, accuracy, inference_time
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

def main():
    print("=" * 60)
    print("ğŸš€ VIETNAMESE CLICKBAIT INFERENCE - CPU TEST")
    print("=" * 60)
    
    # System info
    print(f"ğŸ”§ Python version: {sys.version}")
    print(f"ğŸ”§ PyTorch version: {torch.__version__}")
    print(f"ğŸ”§ CUDA available: {torch.cuda.is_available()}")
    print(f"ğŸ”§ Device: CPU")
    print(f"ğŸ”§ CPU threads: {torch.get_num_threads()}")
    
    # Models to test
    models_to_test = [
        "vinai/phobert-base",
        # Add more models if needed
    ]
    
    print(f"\nğŸ“‹ Testing {len(models_to_test)} model(s)")
    
    # Test 1: Sample texts
    print(f"\n" + "="*50)
    print(f"ğŸ§ª TEST 1: Sample Vietnamese Texts")
    print(f"="*50)
    
    sample_texts, expected_labels = test_sample_texts()
    
    for model_name in models_to_test:
        test_model_inference(model_name, sample_texts, expected_labels)
    
    # Test 2: Real data (if available)
    print(f"\n" + "="*50)
    print(f"ğŸ§ª TEST 2: Real Test Data Subset")
    print(f"="*50)
    
    test_file = os.path.join("data", "test_dtVN", "training_data.jsonl")
    test_texts, test_labels = load_test_data_subset(test_file, n_samples=20)
    
    if test_texts:
        for model_name in models_to_test:
            test_model_inference(model_name, test_texts, test_labels)
    else:
        print("âš ï¸ KhÃ´ng cÃ³ data tháº­t Ä‘á»ƒ test. Cháº¡y scripts/prepare_training_data.py trÆ°á»›c.")
    
    # Summary
    print(f"\n" + "="*60)
    print(f"âœ… CPU INFERENCE TEST COMPLETED")
    print(f"="*60)
    print(f"ğŸ’» Device: CPU")
    print(f"ğŸ”§ Models tested: {len(models_to_test)}")
    print(f"ğŸ“Š Sample texts: {len(sample_texts)}")
    if test_texts:
        print(f"ğŸ“Š Real test texts: {len(test_texts)}")
    
    print(f"\nğŸ’¡ Äá»ƒ cháº¡y training test: python scripts/test_cpu_quick.py")

if __name__ == "__main__":
    main() 