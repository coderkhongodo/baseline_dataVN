# ğŸš€ HÆ°á»›ng dáº«n Fine-tune Clickbait Classification tá»« A-Z

**HÆ°á»›ng dáº«n chi tiáº¿t tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i Ä‘á»ƒ fine-tune cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i clickbait trÃªn GPU RTX A5000**

---

## ğŸ“‹ Má»¥c lá»¥c

1. [Tá»•ng quan Project](#1-tá»•ng-quan-project)
2. [YÃªu cáº§u Há»‡ thá»‘ng](#2-yÃªu-cáº§u-há»‡-thá»‘ng)
3. [CÃ i Ä‘áº·t MÃ´i trÆ°á»ng](#3-cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng)
4. [Chuáº©n bá»‹ Dá»¯ liá»‡u](#4-chuáº©n-bá»‹-dá»¯-liá»‡u)
5. [Cháº¡y Training](#5-cháº¡y-training)
6. [ÄÃ¡nh giÃ¡ Káº¿t quáº£](#6-Ä‘Ã¡nh-giÃ¡-káº¿t-quáº£)
7. [Troubleshooting](#7-troubleshooting)
8. [Best Practices](#8-best-practices)

---

## 1. Tá»•ng quan Project

### ğŸ¯ Má»¥c tiÃªu
Fine-tune cÃ¡c mÃ´ hÃ¬nh state-of-the-art Ä‘á»ƒ phÃ¢n loáº¡i tiÃªu Ä‘á» clickbait vs non-clickbait trÃªn dataset Webis-Clickbait-17.

### ğŸ“Š Dataset
- **Training**: 30,812 samples
- **Validation**: 3,851 samples  
- **Test**: 3,854 samples
- **Format**: `{"id": "abc123", "text": "TiÃªu Ä‘á»", "label": 1}`

### ğŸ¤– MÃ´ hÃ¬nh há»— trá»£
- **BERT Family**: BERT-base, DeBERTa-v3-base, BERT-large
- **Large Language Models**: Mistral-7B, Llama-2-7B, Llama-3-8B, Llama-2-13B
- **Training Methods**: Full fine-tune, LoRA, QLoRA

---

## 2. YÃªu cáº§u Há»‡ thá»‘ng

### ğŸ’» Hardware
```
Minimum:
- GPU: 8GB VRAM (cho BERT-base)
- RAM: 16GB+
- Storage: 50GB free space

Recommended (RTX A5000):
- GPU: 24GB VRAM
- RAM: 32GB+
- Storage: 100GB free space
```

### ğŸ Software
```
- Python: 3.8+
- CUDA: 11.8+ hoáº·c 12.x
- Git
- Windows 10/11 hoáº·c Linux
```

---

## 3. CÃ i Ä‘áº·t MÃ´i trÆ°á»ng

### 3.1 Clone Repository

```bash
git clone <repository-url>
cd clickbait-classification
```

### 3.2 Táº¡o Virtual Environment

**Windows:**
```cmd
python -m venv clickbait_env
clickbait_env\Scripts\activate
```

**Linux/Mac:**
```bash
python -m venv clickbait_env
source clickbait_env/bin/activate
```

### 3.3 CÃ i Ä‘áº·t Dependencies

```bash
# CÃ i Ä‘áº·t PyTorch vá»›i CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n khÃ¡c
pip install -r requirements.txt
```

### 3.4 Kiá»ƒm tra CÃ i Ä‘áº·t

```bash
python scripts/setup_environment.py
```

**Output mong Ä‘á»£i:**
```
ğŸ”§ GPU: NVIDIA GeForce RTX A5000 (24.0 GB)
âœ… Python version is compatible
âœ… All required packages are installed!
âœ… Data structure is correct!
```

---

## 4. Chuáº©n bá»‹ Dá»¯ liá»‡u

### 4.1 Kiá»ƒm tra Dá»¯ liá»‡u

```bash
# Kiá»ƒm tra sá»‘ lÆ°á»£ng samples
wc -l data/train/data.jsonl    # Should be 30812
wc -l data/val/data.jsonl      # Should be 3851  
wc -l data/test/data.jsonl     # Should be 3854

# Xem vÃ i samples Ä‘áº§u tiÃªn
head -n 3 data/train/data.jsonl
```

### 4.2 Cáº¥u trÃºc Dá»¯ liá»‡u
```
data/
â”œâ”€â”€ train/data.jsonl    # Training data
â”œâ”€â”€ val/data.jsonl      # Validation data
â””â”€â”€ test/data.jsonl     # Test data
```

---

## 5. Cháº¡y Training

### 5.1 ğŸŒŸ PhÆ°Æ¡ng phÃ¡p 1: Interactive Guide (Khuyáº¿n nghá»‹)

```bash
python scripts/quick_start_a5000.py
```

**Interactive menu sáº½ hiá»ƒn thá»‹:**
```
ğŸ“‹ AVAILABLE MODELS FOR RTX A5000 (24GB)

ğŸ”¥ BERT Family (Recommended for beginners)
   1. BERT-base-uncased    | 110M params, ~45min, F1â‰ˆ0.70
   2. DeBERTa-v3-base      | 184M params, ~60min, F1â‰ˆ0.72
   3. BERT-large-uncased   | 340M params, ~90min, F1â‰ˆ0.73

ğŸš€ Large Language Models with LoRA
   4. Mistral-7B-v0.2      | 7B params, ~120min, F1â‰ˆ0.72
   5. Mistral-7B-Instruct  | 7B params, ~90min, F1â‰ˆ0.73
   6. Llama-2-7B           | 7B params, ~120min, F1â‰ˆ0.70
   7. Llama-3-8B           | 8B params, ~150min, F1â‰ˆ0.74
   8. Llama-2-13B          | 13B params, ~180min, F1â‰ˆ0.75

   9. All BERT family models
   10. All LLM models  
   11. ALL models (full benchmark)

Enter your choice (1-11):
```

### 5.2 ğŸ”§ PhÆ°Æ¡ng phÃ¡p 2: Manual Training

#### A. Training BERT Family Models

```bash
# Train má»™t model cá»¥ thá»ƒ
python scripts/train_bert_family.py --model bert-base-uncased
python scripts/train_bert_family.py --model deberta-v3-base
python scripts/train_bert_family.py --model bert-large-uncased

# Train táº¥t cáº£ BERT family models
python scripts/train_bert_family.py --model all
```

#### B. Training LLM vá»›i LoRA

```bash
# Train Mistral models
python scripts/train_llm_lora.py --model mistral-7b-v0.2
python scripts/train_llm_lora.py --model mistral-7b-instruct

# Train Llama models  
python scripts/train_llm_lora.py --model llama2-7b
python scripts/train_llm_lora.py --model llama3-8b
python scripts/train_llm_lora.py --model llama2-13b

# Train táº¥t cáº£ LLM models
python scripts/train_llm_lora.py --model all
```

### 5.3 ğŸ PhÆ°Æ¡ng phÃ¡p 3: Full Benchmark Suite

```bash
# Cháº¡y táº¥t cáº£ experiments (12-15 giá»)
python scripts/run_all_experiments.py
```

**LÆ°u Ã½:** Script sáº½ há»i xÃ¡c nháº­n cho tá»«ng experiment:
```
ğŸ¬ EXPERIMENT 1/6: BERT Family Models
Expected duration: ~180 minutes
Models: bert-base-uncased, deberta-v3-base, bert-large-uncased

Proceed with BERT Family Models? (y/n/skip):
```

### 5.4 ğŸ“Š Monitoring Training

**Trong terminal khÃ¡c, monitor GPU usage:**
```bash
# Windows
nvidia-smi -l 1

# Linux  
watch -n 1 nvidia-smi
```

**Training logs sáº½ hiá»ƒn thá»‹:**
```
ğŸš€ TRAINING BERT-BASE-UNCASED
============================================================
ğŸ¤– Loading google-bert/bert-base-uncased...
ğŸ“Š Loading and tokenizing dataset...
Dataset loaded: DatasetDict({
    train: Dataset({features: ['id', 'text', 'label'], num_rows: 30812})
    validation: Dataset({features: ['id', 'text', 'label'], num_rows: 3851})
    test: Dataset({features: ['id', 'text', 'label'], num_rows: 3854})
})

ğŸ‹ï¸ Starting training...
{'train_runtime': 2847.23, 'train_samples_per_second': 43.25, ...}

ğŸ“Š RESULTS FOR BERT-BASE-UNCASED:
   Accuracy: 0.8612
   F1 (weighted): 0.7089
   F1 (macro): 0.7012
   F1 (binary): 0.7089
   Training time: 0h 47m 27s
   Model saved to: outputs/bert-base-uncased-a5000
```

---

## 6. ÄÃ¡nh giÃ¡ Káº¿t quáº£

### 6.1 Generate Comparison Report

```bash
python scripts/benchmark_results.py --save_csv
```

**Output:**
```
ğŸ“Š BENCHMARK RESULTS ANALYSIS
==================================================
ğŸ“ˆ Found results for 8 models

ğŸ† MODEL PERFORMANCE RANKING:
================================================================================
               Model Parameters Training Method  F1 Score  Accuracy Training Time
         llama2-13b       13B     LoRA + 8bit    0.7543    0.8789         3h 12m
        llama3-8b         8B     LoRA + 4bit    0.7421    0.8712         2h 34m
  mistral-7b-instruct     7B     LoRA + 4bit    0.7356    0.8689         1h 32m
   bert-large-uncased    340M  Full Fine-tune    0.7298    0.8656         1h 28m
      deberta-v3-base    184M  Full Fine-tune    0.7203    0.8612         1h 02m
     mistral-7b-v0.2      7B     LoRA + 4bit    0.7156    0.8578         2h 08m
          llama2-7b       7B     LoRA + 4bit    0.7098    0.8534         2h 15m
    bert-base-uncased    110M  Full Fine-tune    0.7089    0.8612         0h 47m

ğŸ’¡ KEY INSIGHTS:
==================================================
ğŸ¥‡ Best Overall: llama2-13b (F1: 0.7543)
ğŸ¥‡ Best LoRA: llama2-13b (F1: 0.7543)
ğŸ¥‡ Best Full Fine-tune: bert-large-uncased (F1: 0.7298)

âœ… Analysis complete! Results saved to outputs/
```

### 6.2 Detailed Model Evaluation

```bash
# Evaluate má»™t model cá»¥ thá»ƒ
python scripts/evaluate_model.py --model_dir outputs/llama2-13b-lora-a5000 --demo

# Output sáº½ bao gá»“m:
# - Detailed metrics
# - Classification report  
# - Confusion matrix
# - Demo inference trÃªn sample texts
```

### 6.3 Inference trÃªn Text Má»›i

```python
from transformers import pipeline

# Load model Ä‘Ã£ train
classifier = pipeline(
    "text-classification",
    model="outputs/llama2-13b-lora-a5000"
)

# Test trÃªn text má»›i
texts = [
    "Báº¡n sáº½ khÃ´ng tin Ä‘iá»u xáº£y ra tiáº¿p theo...",
    "ChÃ­nh phá»§ cÃ´ng bá»‘ káº¿ hoáº¡ch kinh táº¿ má»›i",
    "7 bÃ­ máº­t mÃ  bÃ¡c sÄ© khÃ´ng muá»‘n báº¡n biáº¿t!"
]

for text in texts:
    result = classifier(text)
    print(f"'{text}' -> {result[0]['label']} ({result[0]['score']:.3f})")
```

---

## 7. Troubleshooting

### 7.1 ğŸš¨ Lá»—i thÆ°á»ng gáº·p

#### **CUDA Out of Memory**
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**Giáº£i phÃ¡p:**
```bash
# Giáº£m batch size trong scripts/train_bert_family.py
"batch_size": 16,  # Thay vÃ¬ 32

# Hoáº·c tÄƒng gradient accumulation
"gradient_accumulation_steps": 4,  # Thay vÃ¬ 2

# Hoáº·c dÃ¹ng model nhá» hÆ¡n
python scripts/train_bert_family.py --model bert-base-uncased  # Thay vÃ¬ bert-large
```

#### **Package Import Errors**
```
ModuleNotFoundError: No module named 'transformers'
```

**Giáº£i phÃ¡p:**
```bash
# Äáº£m báº£o virtual environment Ä‘Æ°á»£c activate
source clickbait_env/bin/activate  # Linux/Mac
clickbait_env\Scripts\activate     # Windows

# Reinstall packages
pip install --upgrade -r requirements.txt
```

#### **Model Download Errors**
```
HTTPSConnectionPool: Max retries exceeded
```

**Giáº£i phÃ¡p:**
```bash
# Set proxy náº¿u cáº§n
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# Hoáº·c download manual
from transformers import AutoModel
model = AutoModel.from_pretrained("microsoft/deberta-v3-base", force_download=True)
```

### 7.2 ğŸ”§ Performance Optimization

#### **Äá»ƒ tÄƒng tá»‘c training:**
```python
# Trong training arguments
training_args = TrainingArguments(
    fp16=True,                    # Mixed precision
    dataloader_num_workers=4,     # Parallel data loading
    gradient_checkpointing=True,  # Save memory
    warmup_steps=100,             # Faster convergence
)
```

#### **Äá»ƒ giáº£m memory usage:**
```python
training_args = TrainingArguments(
    per_device_train_batch_size=8,    # Smaller batches
    gradient_accumulation_steps=4,    # Maintain effective batch size
    fp16=True,                        # Half precision
    dataloader_pin_memory=False,      # Save memory
)
```

---

## 8. Best Practices

### 8.1 ğŸ¯ Lá»±a chá»n Model

| Use Case | Recommended Model | Why |
|----------|-------------------|-----|
| **Quick Prototype** | BERT-base-uncased | Nhanh, Ã­t tÃ i nguyÃªn |
| **Best Accuracy** | Llama-2-13B + QLoRA | Performance cao nháº¥t |
| **Balanced** | DeBERTa-v3-base | Tá»‘t vÃ  training nhanh |
| **Production** | Mistral-7B-Instruct | Instruction-tuned, reliable |

### 8.2 âš™ï¸ Training Configuration

#### **Cho RTX A5000 (24GB):**
```python
# BERT Family - Aggressive settings
BERT_CONFIG = {
    "batch_size": 48,
    "learning_rate": 2e-5,
    "epochs": 4,
    "fp16": True
}

# LLM LoRA - Conservative settings  
LLM_CONFIG = {
    "batch_size": 10,
    "learning_rate": 5e-6,
    "epochs": 3,
    "quantization": "4bit"
}
```

#### **Cho GPU nhá» hÆ¡n (8-16GB):**
```python
# Giáº£m batch size vÃ  tÄƒng gradient accumulation
SMALL_GPU_CONFIG = {
    "batch_size": 8,
    "gradient_accumulation_steps": 4,
    "fp16": True
}
```

### 8.3 ğŸ“Š Monitoring & Logging

```bash
# Monitor GPU real-time
nvidia-smi -l 1

# Monitor training logs
tail -f outputs/model-name/trainer_state.json

# Check disk space
df -h outputs/
```

### 8.4 ğŸ’¾ Save Storage Space

```bash
# Chá»‰ keep best checkpoints
training_args = TrainingArguments(
    save_total_limit=1,           # Keep only best checkpoint
    load_best_model_at_end=True,
)

# Clean up sau training
rm -rf outputs/*/checkpoint-*     # Remove intermediate checkpoints
```

### 8.5 ğŸ”„ Experiment Management

```bash
# Organize experiments by date
mkdir outputs/2024-01-15-bert-family
mv outputs/bert-* outputs/2024-01-15-bert-family/

# Log all experiments
python scripts/run_all_experiments.py > experiment_log_$(date +%Y%m%d).txt 2>&1
```

---

## ğŸ‰ HoÃ n thÃ nh!

Sau khi hoÃ n thÃ nh cÃ¡c bÆ°á»›c trÃªn, báº¡n sáº½ cÃ³:

âœ… **Trained Models**: Multiple fine-tuned models trong `outputs/`
âœ… **Benchmark Results**: Comparison table vÃ  visualizations
âœ… **Production-ready Models**: Saved models cÃ³ thá»ƒ deploy
âœ… **Detailed Logs**: Training history vÃ  performance metrics

### ğŸ“ Output Structure
```
outputs/
â”œâ”€â”€ bert-base-uncased-a5000/         # Trained BERT model
â”œâ”€â”€ llama2-13b-lora-a5000/          # Best performing model
â”œâ”€â”€ benchmark_results.csv           # Comparison table
â”œâ”€â”€ benchmark_summary.json          # Summary results
â”œâ”€â”€ experiment_log.json             # Full experiment history
â””â”€â”€ experiment_log_20240115.txt     # Training logs
```

### ğŸš€ Next Steps
1. **Deploy model** cho production
2. **Fine-tune thÃªm** trÃªn domain-specific data
3. **Optimize inference** speed cho real-time applications
4. **A/B test** different models trong production

---

**ğŸ’¡ Need Help?**
- Xem `FINE_TUNING_GUIDE.md` cho advanced topics
- Check GitHub Issues cho community support
- Monitor GPU vá»›i `nvidia-smi` khi training

**Happy Fine-tuning! ğŸ¯** 