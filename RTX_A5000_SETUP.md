# ðŸš€ Vietnamese Clickbait Classification - RTX A5000 Setup

## ðŸ“‹ Tá»•ng quan

Setup hoÃ n chá»‰nh cho huáº¥n luyá»‡n Vietnamese Clickbait Classification trÃªn **2x RTX A5000** (48GB VRAM tá»•ng cá»™ng).

### âœ… ÄÃ£ hoÃ n thÃ nh:
- âœ… **Dá»¯ liá»‡u**: 3,698 samples tiáº¿ng Viá»‡t Ä‘Ã£ Ä‘Æ°á»£c chia stratified (70:15:15)
- âœ… **Preprocessing**: Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t `id`, `title`, `label` 
- âœ… **Configs**: Tá»‘i Æ°u cho 2x RTX A5000 vá»›i multi-GPU training
- âœ… **Models**: PhoBERT, XLM-RoBERTa, Vietnamese LLMs (Vistral, VinaLlama, SeaLLM)
- âœ… **Pipeline**: Script tá»± Ä‘á»™ng tá»« setup Ä‘áº¿n evaluation

## ðŸ–¥ï¸ Hardware Requirements

```
âœ… GPU Memory: 48GB - Äá»§ máº¡nh cho má»i model
âœ… Compute: 8.6 - Há»— trá»£ mixed precision
âœ… Multi-GPU: NCCL backend vá»›i 2 cards
```

### ðŸ“Š Capacity Estimates:
- **PhoBERT-base**: 48 batch/GPU, ~8GB VRAM
- **PhoBERT-large**: 24 batch/GPU, ~12GB VRAM  
- **XLM-RoBERTa-base**: 32 batch/GPU, ~10GB VRAM
- **XLM-RoBERTa-large**: 16 batch/GPU, ~16GB VRAM
- **Vietnamese 7B LLMs**: 4 batch/GPU, ~20GB VRAM

## ðŸŽ¯ Training Strategies

| Strategy | Model | Time | Expected Accuracy | Use Case |
|----------|-------|------|-------------------|----------|
| **QUICK_TEST** | PhoBERT-base | 30 phÃºt | 85-88% | Test setup nhanh |
| **BEST_VIETNAMESE** | PhoBERT-large | 2-3 giá» | 88-91% | Tá»‘t nháº¥t cho tiáº¿ng Viá»‡t |
| **MULTILINGUAL** | XLM-RoBERTa-large | 3-4 giá» | 85-89% | Multilingual support |
| **LLM_LORA** | Vistral-7B | 4-6 giá» | 89-92% | State-of-the-art |

## ðŸš€ Quick Start

### 1. Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ chuáº©n bá»‹
```bash
# Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chia vÃ  chuáº©n bá»‹
ls data/train_dtVN/training_data.jsonl
ls data/val_dtVN/training_data.jsonl  
ls data/test_dtVN/training_data.jsonl
```

### 2. Test nhanh vá»›i PhoBERT-base (30 phÃºt)
```bash
python scripts/run_rtx_a5000_training.py \\
    --models phobert-base \\
    --strategy fast
```

### 3. Training tá»‘t nháº¥t cho tiáº¿ng Viá»‡t (2-3 giá»)
```bash
python scripts/run_rtx_a5000_training.py \\
    --models phobert-large \\
    --strategy balanced
```

### 4. Multi-model comparison 
```bash
python scripts/run_rtx_a5000_training.py \\
    --models phobert-base phobert-large xlm-roberta-large \\
    --strategy balanced
```

### 5. State-of-the-art vá»›i LLM (4-6 giá»)
```bash
python scripts/run_rtx_a5000_training.py \\
    --models vistral-7b \\
    --strategy thorough
```

## ðŸ”§ Manual Training

### Cháº¡y tá»«ng bÆ°á»›c:

#### 1. Chuáº©n bá»‹ dá»¯ liá»‡u (Ä‘Ã£ xong)
```bash
python scripts/prepare_training_data.py
```

#### 2. Training riÃªng láº»
```bash
# PhoBERT-base
python scripts/train_vietnamese_rtx_a5000.py \\
    --model-type phobert-base \\
    --strategy balanced \\
    --test-samples

# PhoBERT-large  
python scripts/train_vietnamese_rtx_a5000.py \\
    --model-type phobert-large \\
    --strategy balanced \\
    --test-samples

# XLM-RoBERTa-large
python scripts/train_vietnamese_rtx_a5000.py \\
    --model-type xlm-roberta-large \\
    --strategy balanced \\
    --test-samples

# Vistral-7B vá»›i LoRA
python scripts/train_vietnamese_rtx_a5000.py \\
    --model-type vistral-7b \\
    --strategy balanced \\
    --test-samples
```

## ðŸ“Š Data Overview

### Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹:
```
ðŸ“Š PhÃ¢n phá»‘i label:
  Train: clickbait 922 (35.6%), non-clickbait 1666 (64.4%)
  Val:   clickbait 197 (35.5%), non-clickbait 358 (64.5%)  
  Test:  clickbait 198 (35.7%), non-clickbait 357 (64.3%)

ðŸ“ Thá»‘ng kÃª Ä‘á»™ dÃ i title:
  Trung bÃ¬nh: 66.6 kÃ½ tá»±
  95%: 103 kÃ½ tá»±
  Max: 194 kÃ½ tá»±
  Äá» xuáº¥t max_length: 154 tokens
```

### Cáº¥u trÃºc dá»¯ liá»‡u:
```json
{
  "id": "article_xxxx",
  "title": "TiÃªu Ä‘á» bÃ i bÃ¡o tiáº¿ng Viá»‡t",
  "label": "clickbait" hoáº·c "non-clickbait"
}
```

## âš¡ Performance Optimizations

### ÄÃ£ Ä‘Æ°á»£c cáº¥u hÃ¬nh:
- âœ… **Mixed Precision (FP16)**: TÄƒng tá»‘c 1.5-2x
- âœ… **Gradient Checkpointing**: Cho large models
- âœ… **Multi-GPU Training**: NCCL backend
- âœ… **Optimized Batch Sizes**: PhÃ¹ há»£p vá»›i tá»«ng model
- âœ… **Learning Rate Scheduling**: Linear/Cosine
- âœ… **Early Stopping**: TrÃ¡nh overfitting

## ðŸ“ Output Structure

```
models/vietnamese_clickbait/
â”œâ”€â”€ phobert-base_20250623_143022_balanced/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ training_config.json
â”‚   â”œâ”€â”€ final_results.json
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ phobert-large_20250623_150145_balanced/
â””â”€â”€ training_summary_20250623.json
```

## ðŸ§ª Testing Models

### Test vá»›i sample predictions:
```bash
python scripts/train_vietnamese_rtx_a5000.py \\
    --model-type phobert-base \\
    --strategy fast \\
    --test-samples
```

### Sample test cases:
```
1. "BÃ­ máº­t kinh hoÃ ng mÃ  bÃ¡c sÄ© khÃ´ng bao giá» tiáº¿t lá»™!" -> clickbait
2. "NghiÃªn cá»©u má»›i vá» tÃ¡c Ä‘á»™ng cá»§a caffeine Ä‘áº¿n sá»©c khá»e" -> non-clickbait  
3. "CÃ¡ch lÃ m giÃ u nhanh trong 30 ngÃ y mÃ  ai cÅ©ng cÃ³ thá»ƒ lÃ m" -> clickbait
4. "BÃ¡o cÃ¡o kinh táº¿ quÃ½ II nÄƒm 2025 cá»§a NgÃ¢n hÃ ng Tháº¿ giá»›i" -> non-clickbait
5. "Sá»‘c: PhÃ¡t hiá»‡n bÃ­ máº­t Ä‘á»™ng trá»i vá» sao Viá»‡t ná»•i tiáº¿ng!" -> clickbait
```

## ðŸ” Monitoring & Logs

### Training logs:
- `training.log`: Chi tiáº¿t quÃ¡ trÃ¬nh training
- `rtx_a5000_training.log`: Pipeline logs
- `models/.../logs/`: TensorBoard logs

### TensorBoard:
```bash
tensorboard --logdir models/vietnamese_clickbait/phobert-base_*/logs/
```

## ðŸš¨ Troubleshooting

### Common Issues:

#### 1. CUDA Out of Memory
```bash
# Giáº£m batch size trong configs/rtx_a5000_configs.py
# Hoáº·c sá»­ dá»¥ng strategy="fast"
```

#### 2. Multi-GPU khÃ´ng hoáº¡t Ä‘á»™ng
```bash
# Kiá»ƒm tra:
python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}')"

# Náº¿u cáº§n, cháº¡y single GPU:
CUDA_VISIBLE_DEVICES=0 python scripts/train_vietnamese_rtx_a5000.py ...
```

#### 3. Dependencies missing
```bash
pip install -r requirements_vietnamese.txt
```

## ðŸŽ¯ Expected Results

### Performance Benchmarks:

| Model | Accuracy | F1-Score | Training Time | VRAM Usage |
|-------|----------|----------|---------------|------------|
| PhoBERT-base | 85-88% | 0.83-0.86 | 30-45 min | ~8GB/GPU |
| PhoBERT-large | 88-91% | 0.86-0.90 | 2-3 hours | ~12GB/GPU |
| XLM-RoBERTa-large | 85-89% | 0.83-0.87 | 3-4 hours | ~16GB/GPU |
| Vistral-7B (LoRA) | 89-92% | 0.87-0.91 | 4-6 hours | ~20GB/GPU |

### Success Criteria:
- âœ… F1-Score > 0.85 cho production
- âœ… Accuracy > 87% cho competitive performance  
- âœ… Training time < 6 hours
- âœ… Memory usage < 24GB/GPU

## ðŸ“ž Support

### Quick Commands:
```bash
# Show hardware info
python scripts/run_rtx_a5000_training.py --show-hardware

# Show strategies
python scripts/run_rtx_a5000_training.py --show-strategies

# Check data
python scripts/prepare_training_data.py --data-dir data

# Test config
python configs/rtx_a5000_configs.py
```

---

## ðŸŽ‰ Ready to Train!

Há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c setup hoÃ n chá»‰nh vÃ  tá»‘i Æ°u cho 2x RTX A5000. Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u training ngay:

```bash
# Test nhanh 30 phÃºt:
python scripts/run_rtx_a5000_training.py --models phobert-base --strategy fast

# Hoáº·c training full vá»›i PhoBERT-large (2-3 giá»):
python scripts/run_rtx_a5000_training.py --models phobert-large --strategy balanced
``` 