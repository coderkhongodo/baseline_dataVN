# ğŸ‡»ğŸ‡³ Vietnamese Clickbait Classification using LLM Fine-tuning

## Tá»•ng Quan

Dá»± Ã¡n nÃ y triá»ƒn khai há»‡ thá»‘ng phÃ¢n loáº¡i clickbait tiáº¿ng Viá»‡t sá»­ dá»¥ng Large Language Models (LLMs) vÃ  BERT family models. Há»‡ thá»‘ng Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘áº·c biá»‡t cho tiáº¿ng Viá»‡t vá»›i cÃ¡c ká»¹ thuáº­t preprocessing, word segmentation vÃ  prompting phÃ¹ há»£p.

### âœ¨ TÃ­nh NÄƒng ChÃ­nh

- **ğŸ‡»ğŸ‡³ Tá»‘i Æ°u cho tiáº¿ng Viá»‡t**: Há»— trá»£ PhoBERT, XLM-RoBERTa vÃ  cÃ¡c Vietnamese LLMs
- **ğŸ“Š Word Segmentation**: TÃ­ch há»£p Underthesea vÃ  PyVi cho tiáº¿ng Viá»‡t
- **ğŸ¯ Multiple Approaches**: Fine-tuning BERT + Prompting vá»›i LLMs
- **ğŸ“ˆ Comprehensive Evaluation**: Metrics chi tiáº¿t vÃ  visualization
- **ğŸ”§ Hardware Optimized**: Cáº¥u hÃ¬nh cho RTX 4090/3080/3060

### ğŸ† Models ÄÆ°á»£c Há»— Trá»£

#### BERT Family
- **PhoBERT-base/large**: BERT Ä‘Æ°á»£c train trÃªn corpus tiáº¿ng Viá»‡t
- **XLM-RoBERTa-base/large**: Multilingual model há»— trá»£ tiáº¿ng Viá»‡t
- **mBERT**: BERT multilingual chuáº©n

#### Vietnamese LLMs (vá»›i LoRA)
- **Vistral-7B**: Vietnamese adaptation cá»§a Mistral 7B
- **VinaLLaMA-7B**: Vietnamese version cá»§a LLaMA 7B
- **SeaLLM-7B**: Southeast Asian LLM
- **Gemma-7B**: Google's multilingual model

## ğŸš€ Quick Start

### 1. CÃ i Äáº·t MÃ´i TrÆ°á»ng

```bash
# Clone repository
git clone <your-repo>
cd clickbait-classification-LLM

# CÃ i Ä‘áº·t dependencies cho Vietnamese
pip install -r requirements_vietnamese.txt

# Setup Vietnamese environment
python scripts/setup_vietnamese_environment.py
```

### 2. Chuáº©n Bá»‹ Dá»¯ Liá»‡u Vietnamese

```bash
# Preprocessing dá»¯ liá»‡u tiáº¿ng Viá»‡t
python scripts/preprocess_vietnamese_data.py \
    --input your_vietnamese_data.jsonl \
    --output data_vietnamese \
    --segment underthesea
```

**Format dá»¯ liá»‡u Ä‘áº§u vÃ o:**
```json
{"text": "TiÃªu Ä‘á» tin tá»©c tiáº¿ng Viá»‡t", "label": 0}
{"text": "Báº¡n sáº½ khÃ´ng tin Ä‘Æ°á»£c Ä‘iá»u nÃ y!", "label": 1}
```

### 3. Training Models

#### Option A: Cháº¡y Pipeline HoÃ n Chá»‰nh
```bash
python scripts/run_vietnamese_pipeline.py \
    --input_data your_vietnamese_data.jsonl \
    --models phobert-base xlm-roberta-base \
    --hardware rtx_4090
```

#### Option B: Training Tá»«ng BÆ°á»›c

**Train BERT Models:**
```bash
python scripts/train_vietnamese_bert.py \
    --model phobert-base \
    --hardware rtx_4090
```

**Prompting Evaluation:**
```bash
python scripts/vietnamese_prompting.py
```

### 4. Evaluation

```bash
# ÄÃ¡nh giÃ¡ táº¥t cáº£ models
python scripts/evaluate_vietnamese_models.py --compare_all

# ÄÃ¡nh giÃ¡ model cá»¥ thá»ƒ
python scripts/evaluate_vietnamese_models.py \
    --model_path outputs_vietnamese/phobert-base-vietnamese
```

## ğŸ“Š Cáº¥u TrÃºc Dá»¯ Liá»‡u

```
data_vietnamese/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ data.jsonl          # Full training data
â”‚   â””â”€â”€ data_demo.jsonl     # Demo subset
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ data.jsonl          # Validation data
â”‚   â””â”€â”€ data_demo.jsonl
â””â”€â”€ test/
    â”œâ”€â”€ data.jsonl          # Test data
    â””â”€â”€ data_demo.jsonl
```

## ğŸ¤– Models Configuration

### Hardware Requirements

| Hardware | Memory | Recommended Models | Batch Size |
|----------|---------|-------------------|------------|
| RTX 4090 | 24GB   | PhoBERT-large, Vistral-7B | Full |
| RTX 3080 | 10GB   | PhoBERT-base, XLM-RoBERTa | Medium |
| RTX 3060 | 8GB    | PhoBERT-base | Small |
| CPU Only | 32GB+  | PhoBERT-base | Very Small |

### Model-Specific Settings

```python
# PhoBERT - Cáº§n word segmentation
{
    "preprocessing": "word_segmentation",
    "batch_size": 32,
    "learning_rate": 2e-5,
    "max_length": 256
}

# XLM-RoBERTa - KhÃ´ng cáº§n word segmentation
{
    "preprocessing": "none", 
    "batch_size": 24,
    "learning_rate": 2e-5,
    "max_length": 256
}
```

## ğŸ¯ Prompting Methods

### Zero-Shot
```python
prompt = """PhÃ¢n loáº¡i tiÃªu Ä‘á» tin tá»©c tiáº¿ng Viá»‡t:
- 0: KhÃ´ng pháº£i clickbait (thÃ´ng tin rÃµ rÃ ng, khÃ¡ch quan)
- 1: Clickbait (giáº­t tÃ­t, gÃ¢y tÃ² mÃ², phÃ³ng Ä‘áº¡i)

TiÃªu Ä‘á»: "{title}"
Tráº£ lá»i: [0/1]"""
```

### Few-Shot
```python
prompt = """Dá»±a trÃªn cÃ¡c vÃ­ dá»¥:
"Thá»§ tÆ°á»›ng kÃ½ nghá»‹ Ä‘á»‹nh má»›i" â†’ 0
"Báº¡n sáº½ sá»‘c khi biáº¿t Ä‘iá»u nÃ y!" â†’ 1

TiÃªu Ä‘á»: "{title}"
Tráº£ lá»i: [0/1]"""
```

### Chain-of-Thought
```python
prompt = """PhÃ¢n tÃ­ch tá»«ng bÆ°á»›c:
1. Tá»« khÃ³a cáº£m xÃºc
2. TÃ­nh cá»¥ thá»ƒ cá»§a thÃ´ng tin  
3. Khoáº£ng trá»‘ng tÃ² mÃ²
4. Káº¿t luáº­n: [0/1]"""
```

## ğŸ“ˆ Evaluation Metrics

- **Accuracy**: Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ
- **F1-Score**: F1 weighted vÃ  per-class
- **Precision/Recall**: Cho tá»«ng class
- **ROC AUC**: Area under curve
- **Confusion Matrix**: Ma tráº­n nháº§m láº«n

## ğŸ”§ Customization

### ThÃªm Vietnamese Patterns

```python
# Trong preprocess_vietnamese_data.py
def detect_clickbait_patterns(self, text: str):
    vietnamese_patterns = {
        'emotional_words': any(word in text.lower() for word in [
            'sá»‘c', 'choÃ¡ng', 'kinh hoÃ ng', 'báº¥t ngá»',
            'tuyá»‡t vá»i', 'hoÃ n háº£o', 'xuáº¥t sáº¯c'
        ]),
        'curiosity_phrases': any(phrase in text.lower() for phrase in [
            'báº¡n sáº½ khÃ´ng tin', 'Ä‘iá»u xáº£y ra tiáº¿p theo',
            'bÃ­ máº­t', 'cÃ¡ch', 'thá»§ thuáº­t'
        ])
    }
    return vietnamese_patterns
```

### Custom Model Config

```python
# Trong configs/vietnamese_models.py
CUSTOM_MODEL = {
    "model_name": "your-vietnamese-model",
    "batch_size": 16,
    "learning_rate": 2e-5,
    "preprocessing": "word_segmentation",
    "language": "vietnamese"
}
```

## ğŸ“‹ Examples

### Inference vá»›i Trained Model

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import underthesea

# Load model
model_path = "outputs_vietnamese/phobert-base-vietnamese"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Preprocess Vietnamese text
text = "Báº¡n sáº½ khÃ´ng tin Ä‘Æ°á»£c Ä‘iá»u nÃ y!"
segmented = underthesea.word_tokenize(text, format="text")

# Predict
inputs = tokenizer(segmented, return_tensors="pt")
outputs = model(**inputs)
prediction = outputs.logits.argmax(-1).item()

print(f"Prediction: {'Clickbait' if prediction == 1 else 'KhÃ´ng clickbait'}")
```

### Batch Prediction

```python
vietnamese_headlines = [
    "ChÃ­nh phá»§ thÃ´ng qua nghá»‹ Ä‘á»‹nh má»›i vá» thuáº¿",
    "7 bÃ­ máº­t mÃ  bÃ¡c sÄ© khÃ´ng muá»‘n báº¡n biáº¿t!",
    "GiÃ¡ vÃ ng tÄƒng 2% trong phiÃªn hÃ´m nay",
    "CÃ¡ch lÃ m giÃ u 99% ngÆ°á»i Viá»‡t chÆ°a biáº¿t"
]

predictions = []
for headline in vietnamese_headlines:
    # Preprocess and predict
    segmented = underthesea.word_tokenize(headline, format="text")
    inputs = tokenizer(segmented, return_tensors="pt")
    outputs = model(**inputs)
    pred = outputs.logits.argmax(-1).item()
    predictions.append(pred)

print("Predictions:", predictions)
```

## ğŸ“š Vietnamese NLP Resources

### Libraries
- **Underthesea**: Vietnamese NLP toolkit
- **PyVi**: Vietnamese text processing
- **VnCoreNLP**: Vietnamese core NLP toolkit

### Models
- **PhoBERT**: Vietnamese BERT (VinAI Research)
- **XLM-RoBERTa**: Facebook's multilingual model
- **Vistral**: Vietnamese Mistral adaptation

### Datasets
- **VietNewsCorpus**: Vietnamese news corpus
- **VLSP**: Vietnamese Language and Speech Processing
- **UIT-ViSum**: Vietnamese text summarization

## ğŸ› Troubleshooting

### Common Issues

**1. Word Segmentation Error**
```bash
# Fix: Reinstall underthesea
pip uninstall underthesea
pip install underthesea
```

**2. CUDA Out of Memory**
```bash
# Solution: Reduce batch size
--hardware rtx_3060  # Automatically reduces batch size
```

**3. Model Loading Error**
```bash
# Fix: Check model path
ls outputs_vietnamese/  # List available models
```

**4. Vietnamese Character Encoding**
```python
# Ensure UTF-8 encoding
with open(file_path, 'r', encoding='utf-8') as f:
    data = f.read()
```

## ğŸ“ Vietnamese Text Preprocessing

### Word Segmentation Comparison

| Method | Speed | Accuracy | Use Case |
|--------|-------|----------|----------|
| Underthesea | Medium | High | PhoBERT, research |
| PyVi | Fast | Medium | Quick processing |
| None | Fastest | N/A | XLM-RoBERTa |

### Text Normalization

```python
# Vietnamese-specific normalization
def normalize_vietnamese(text):
    # Unicode normalization
    text = unicodedata.normalize('NFC', text)
    
    # Remove tone marks if needed
    text = unidecode(text)  # Optional
    
    # Clean punctuation
    text = re.sub(r'[!]{2,}', '!', text)
    text = re.sub(r'[?]{2,}', '?', text)
    
    return text
```

## ğŸ Results & Performance

### Expected Performance (Vietnamese)

| Model | Accuracy | F1-Score | Training Time |
|-------|----------|----------|---------------|
| PhoBERT-base | 0.85-0.90 | 0.84-0.89 | 2-3 hours |
| XLM-RoBERTa | 0.82-0.87 | 0.81-0.86 | 3-4 hours |
| Prompting (GPT-4) | 0.80-0.85 | 0.79-0.84 | Few minutes |

### Vietnamese-Specific Challenges

1. **Word Segmentation**: Tiáº¿ng Viá»‡t khÃ´ng cÃ³ space giá»¯a tá»« ghÃ©p
2. **Tone Marks**: 5 dáº¥u thanh khÃ¡c nhau
3. **Slang & Internet Language**: NgÃ´n ngá»¯ máº¡ng xÃ£ há»™i
4. **Regional Variations**: KhÃ¡c biá»‡t miá»n Báº¯c/Nam

## ğŸ¤ Contributing

### Add New Vietnamese Model

1. ThÃªm config vÃ o `configs/vietnamese_models.py`
2. Update training script
3. Test vá»›i Vietnamese data
4. Add evaluation

### Improve Vietnamese Preprocessing

1. Fork repository
2. Enhance `preprocess_vietnamese_data.py`
3. Test vá»›i multiple Vietnamese datasets
4. Submit pull request

## ğŸ“œ License & Citation

```bibtex
@article{vietnamese_clickbait_2024,
  title={Vietnamese Clickbait Classification using LLM Fine-tuning},
  author={Your Name},
  year={2024},
  journal={Vietnamese NLP Research}
}
```

## ğŸ”— Resources

- [PhoBERT Paper](https://arxiv.org/abs/2003.00744)
- [Underthesea Documentation](https://underthesea.readthedocs.io/)
- [Vietnamese NLP Resources](https://github.com/vietnlp/vietnlp)
- [VinAI Research](https://www.vinai.io/)

---

**ğŸ‡»ğŸ‡³ ChÃºc báº¡n thÃ nh cÃ´ng vá»›i Vietnamese Clickbait Classification!** 